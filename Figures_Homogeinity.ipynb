{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "import figure_utils\n",
    "# import reload\n",
    "from importlib import reload\n",
    "\n",
    "project_id = 'K9'\n",
    "project_dict = figure_utils.get_project_dict(project_id)[0]\n",
    "num_clusters = 20\n",
    "\n",
    "clusters, clusters_ID_list = figure_utils.get_clusters(\n",
    "        project_dict, num_clusters\n",
    "    )\n",
    "for cluster_ID in clusters_ID_list:\n",
    "    cluster = clusters[cluster_ID]\n",
    "    print(f'Processing cluster {cluster_ID}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating homogeinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Calculate individuaL results into a single file for each cluster_ID\n",
    "import os\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "import figure_utils\n",
    "# import reload\n",
    "from importlib import reload\n",
    "\n",
    "reload(figure_utils)\n",
    "\n",
    "cluster_number_list = [20,40,60,80,100,120,140,160,180,200,220,240,260,280,300]\n",
    "\n",
    "project_dict = figure_utils.get_project_dict('K9')[0]\n",
    "folder_to_save = r\"C:\\github\\spatially_constrained_spectral_clustering\\results\"\n",
    "# iterate over the number of clusters\n",
    "for number_of_clusters in cluster_number_list:\n",
    "    clusters,clusters_ID_list = figure_utils.get_clusters(project_dict, number_of_clusters)\n",
    "\n",
    "    for cluster_ID in clusters_ID_list:\n",
    "        # create file name\n",
    "        file_name = folder_to_save + '\\\\' + 'homogeneity_' + str(number_of_clusters) + '_' + str(int(cluster_ID)).zfill(3) + '.csv'\n",
    "        output_file = os.path.join(folder_to_save, file_name)\n",
    "        \n",
    "        # load results if exist\n",
    "        if os.path.exists(file_name):\n",
    "            # skip the file if it exists\n",
    "            print('File already exists: ' + file_name)\n",
    "            continue\n",
    "            # results = pd.read_csv(file_name)\n",
    "        else:\n",
    "            results = pd.DataFrame(columns=['sub_N', 'average_corr', 'n_corr'])\n",
    "            # save empty file\n",
    "            results.to_csv(file_name, index=False)\n",
    "        \n",
    "        vox_list = clusters[cluster_ID]\n",
    "        for sub_N in project_dict['Participants']:\n",
    "            # print progress\n",
    "            print('Processing subject ' + str(sub_N) + ' cluster ' + str(cluster_ID) + ' with ' + str(number_of_clusters) + ' clusters')\n",
    "            # check if the subject is already in the results\n",
    "            if sub_N in results['sub_N'].values:\n",
    "                continue\n",
    "            # get the average correlation and number of correlations\n",
    "            average_corr, n_corr, datafolder = figure_utils.get_homogeinity(project_dict,sub_N,vox_list)\n",
    "            row = pd.DataFrame([[sub_N, average_corr, n_corr]], columns=['sub_N', 'average_corr', 'n_corr'])\n",
    "            results = pd.concat([results, row], ignore_index=True)\n",
    "\n",
    "        # determine folder to save\n",
    "        # folder_to_save = os.path.join(datafolder, project_dict['Dataset'], 'results', 'homogeneity')\n",
    "\n",
    "        # create folder if it does not exist\n",
    "        if not os.path.exists(folder_to_save):\n",
    "            os.makedirs(folder_to_save)\n",
    "\n",
    "        # save the results\n",
    "        results.to_csv(output_file, index=False)\n",
    "        print('Results saved in: ' + os.path.join(folder_to_save, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Join individual results into a single file\n",
    "cluster_number_list = [20,40,60,80,100,120,140,160,180,200,220,240,260,280,300]\n",
    "database = 'K9'\n",
    "\n",
    "project_dict = figure_utils.get_project_dict(database)[0]\n",
    "\n",
    "\n",
    "experiment = 'CAPS'\n",
    "folder_to_save = r\"C:\\github\\spatially_constrained_spectral_clustering\\results\\cluster_homogeinity\"\n",
    "input_folder = r\"C:\\github\\spatially_constrained_spectral_clustering\\results\\homogeneity\"\n",
    "\n",
    "# create an empty dataframe to store the results\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for number_of_clusters in cluster_number_list:\n",
    "    clusters,clusters_ID_list = figure_utils.get_clusters(project_dict, number_of_clusters)\n",
    "    for cluster_ID in clusters_ID_list:\n",
    "        # print progress\n",
    "        print('Processing cluster ' + str(cluster_ID) + ' with ' + str(number_of_clusters) + ' clusters')\n",
    "        # create file name\n",
    "        file_name = input_folder + '\\\\' + 'homogeneity_' + str(number_of_clusters) + '_' + str(int(cluster_ID)).zfill(3) + '.csv'\n",
    "        # load results\n",
    "        results = pd.read_csv(file_name)\n",
    "        print('Loaded results from: ' + file_name)\n",
    "        # add column with number of clusters\n",
    "        results['Number of clusters'] = number_of_clusters\n",
    "        # add column with cluster ID\n",
    "        results['Cluster ID'] = cluster_ID\n",
    "        # add results to the dataframe\n",
    "        data = pd.concat([data, results], ignore_index=True)\n",
    "\n",
    "# save data to csv\n",
    "data.to_csv(os.path.join(folder_to_save, 'homogeneity_' + database + '.csv'), index=False)\n",
    "# print saved file\n",
    "print('Results saved in: ' + os.path.join(folder_to_save, 'homogeneity_' + database + '.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Generate slices for the Figure\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy  # Added import\n",
    "from importlib import reload\n",
    "import figure_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable, get_cmap\n",
    "#matplotlib.colormaps.get_cmap(obj)\n",
    "# from matplotlib.colormaps import get_cmap\n",
    "# import ListedColormap\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "reload(figure_utils)\n",
    "\n",
    "# Add options\n",
    "zero_transparent = True  # If True, zeros in data_map and slice_base are set to np.nan\n",
    "nan_transparent = True   # If True, NaNs in data_map are plotted as transparent; if False, print \"decision pending\"\n",
    "alpha = 1.0               # Transparency of overlay\n",
    "plot_type = 'm'  # 'm' for mean' or 'z' for Z-score\n",
    "variable_name = 'Homogeneity'  # 'Homogeneity' \n",
    "write_file=True\n",
    "clear_background=True\n",
    "alphabetic_slides = True\n",
    "min_val_assigned = [] # assign number to override the values in the colormap\n",
    "max_val_assigned = [] # assign number to override the values in the colormap\n",
    "\n",
    "specie = 'D'\n",
    "\n",
    "results_folder = r\"C:\\github\\spatially_constrained_spectral_clustering\\results\\cluster_homogeinity\"\n",
    "results_output_folder = r\"C:\\github\\spatially_constrained_spectral_clustering\\results\\Figures\"\n",
    "results_file = os.path.join(results_folder, 'homogeneity_K9.csv')\n",
    "# load the results\n",
    "results = pd.read_csv(results_file)\n",
    "\n",
    "# To calculate min and max values for the colormap\n",
    "res_by_n_clusters = pd.DataFrame(columns=['number_of_clusters', 'average_corr', 'nan_count'])\n",
    "# get number of clusters\n",
    "cluster_possible_list = results['Number of clusters'].unique()\n",
    "for N_clusters in cluster_possible_list:\n",
    "    # Create a figure according to figure_dims\n",
    "    res_df_sel = results[results['Number of clusters'] == N_clusters]\n",
    "    new_row = {\n",
    "        'number_of_clusters': N_clusters,\n",
    "        'average_corr': np.nanmean(res_df_sel['average_corr'].values),\n",
    "        'nan_count': np.sum(np.isnan(res_df_sel['average_corr'].values)),\n",
    "    }\n",
    "    # add new_row to res_by_n_clusters\n",
    "    res_by_n_clusters = pd.concat([res_by_n_clusters, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# check if value is assigned to min_val_assigned\n",
    "if min_val_assigned: # if min_val_assigned is not empty, set to the assigned value\n",
    "    min_val = min_val_assigned\n",
    "else: # if min_val_assigned is empty, calculate the minimum value\n",
    "    min_val = res_by_n_clusters['average_corr'].min()\n",
    "# check if value is assigned to max_val_assigned\n",
    "if max_val_assigned: # if max_val_assigned is not empty, set to the assigned value\n",
    "    max_val = max_val_assigned # \n",
    "else:\n",
    "    max_val = res_by_n_clusters['average_corr'].max()\n",
    "\n",
    "project_dict = figure_utils.get_project_dict('K9')[0]\n",
    "figure_letter = 'B'\n",
    "figures_folder = figure_utils.get_path('Figures', project_dict, figure_letter=figure_letter)\n",
    "\n",
    "nii_folder = os.path.join(results_output_folder, \"Figure_\" + figure_letter)\n",
    "\n",
    "img_type = \"b_GreyMatter2mm\"\n",
    "base_img = \"Czeibert_brain2mm\"\n",
    "atlas_type = project_dict['Atlas_type']\n",
    "cmap_type = \"hot\" # takes hot \n",
    "\n",
    "alphabetic_slides=False\n",
    "\n",
    "cluster_list = [20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240, 260, 280, 300]\n",
    "\n",
    "for number_of_clusters in cluster_list:\n",
    "    # for number_of_clusters in cluster_list:\n",
    "    segment_map_path = os.path.join(\n",
    "        figure_utils.get_datafolder(username, local_data=True),\n",
    "        project_dict['Dataset'],\n",
    "        'hierarchical_clustering',\n",
    "        f\"{specie}-group-tcorr-{img_type}_{number_of_clusters}.nii.gz\"\n",
    "    )\n",
    "    results_sel = results[results['Number of clusters'] == number_of_clusters]\n",
    "    \n",
    "    # load data_map\n",
    "    data_map = nib.load(segment_map_path).get_fdata()\n",
    "    # Convert data_map to float if necessary\n",
    "    data_map = data_map.astype(float)\n",
    "    \n",
    "    cluster_ID_list = np.unique(data_map)\n",
    "    # remove 0 from cluster_ID_list\n",
    "    cluster_ID_list = cluster_ID_list[cluster_ID_list != 0]\n",
    "    # Go through the cluster_ID_list\n",
    "    for cluster_ID in cluster_ID_list:\n",
    "        # filter results_sel for the cluster_ID\n",
    "        results_sel_cluster = results_sel[results_sel['Cluster ID'] == cluster_ID]\n",
    "        val = np.mean(results_sel_cluster['average_corr'])\n",
    "        # if val is NaN, set it to 0\n",
    "        if np.isnan(val):\n",
    "            val = 0\n",
    "            # print warning\n",
    "            print(f\"Warning: Cluster {cluster_ID} has NaN value, setting it to 0.\")\n",
    "        # assign val to each voxel in data_map that belongs to the cluster_ID\n",
    "        data_map[data_map == cluster_ID] = val\n",
    "    # get unique values in data_map\n",
    "    unique_vals = np.unique(data_map)\n",
    "    # check that data_map doesn't contain NaNs\n",
    "    if np.isnan(data_map).any():\n",
    "        # print(\"Warning: data_map contains NaNs\")\n",
    "        # trigger exception, stop program\n",
    "        raise Exception(\"data_map contains NaNs. Please check the data.\")\n",
    "\n",
    "    # Define the colormap matplotlib.colormaps.get_cmap(obj)\n",
    "    cmap = get_cmap(cmap_type)  # Choose your colormap, e.g., 'viridis', 'plasma', 'coolwarm', etc.\n",
    "    norm = Normalize(vmin=min_val, vmax=max_val)\n",
    "\n",
    "    # Create a scalar map to map values to colors\n",
    "    scalar_map = ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "    # Map unique_vals to colors\n",
    "    cmap_overlay = scalar_map.to_rgba(unique_vals)\n",
    "\n",
    "    # add an initial row that represents alpha = 0\n",
    "    cmap_overlay = np.vstack(([[0, 0, 0, 0]], cmap_overlay))\n",
    "    cmap_overlay = ListedColormap(cmap_overlay)\n",
    "\n",
    "    slice_dict = {\n",
    "        'atlas_type': 'Nitzsche',\n",
    "        'z': [32],\n",
    "        'cmap': cmap_overlay,\n",
    "        'specie': 'D',\n",
    "    }\n",
    "\n",
    "    prefix_out = 'Hom_' + str(number_of_clusters)\n",
    "\n",
    "\n",
    "    figure_utils.plot_slice2(project_dict, prefix_out, figure_letter, slice_dict, data_map, alpha, alphabetic_slides, write_file, clear_background)\n",
    "\n",
    "    # save data_map as nifti file\n",
    "    nii = nib.Nifti1Image(data_map, affine=np.eye(4))\n",
    "    # check if the folder exists, if not create it\n",
    "    if not os.path.exists(nii_folder):\n",
    "        os.makedirs(nii_folder)\n",
    "        # print message\n",
    "        print(f\"Folder {nii_folder} created\")\n",
    "    # save the nii file\n",
    "    nib.save(nii, os.path.join(nii_folder, f\"{specie}_cluster_{number_of_clusters}.nii.gz\"))\n",
    "    # print message\n",
    "    print(f\"File saved in {os.path.join(nii_folder, f'{specie}_cluster_{number_of_clusters}.nii.gz')}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure B - Homogeneity, by number of clusters\n",
    "\n",
    "# Clear all the variables\n",
    "%reset -f\n",
    "import os\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import figure_utils  # Ensure this is correctly imported or defined\n",
    "from scipy.special import comb\n",
    "from importlib import reload\n",
    "from matplotlib import pyplot as plt\n",
    "reload(figure_utils)\n",
    "import matplotlib.ticker as mticker   # ← new import\n",
    "\n",
    "figure_letter = 'B'\n",
    "grid_spacing = 0.03\n",
    "\n",
    "save_file = True\n",
    "second_axis = False  # If True, add a second axis with the number of NaNs\n",
    "color_comb = 'white' # tales 'white' or 'black'\n",
    "\n",
    "# set custom limits\n",
    "\n",
    "# set healthy to 0.40 and 0.70\n",
    "custom_min_y = 0.05 # set number or NaN\n",
    "custom_max_y = 0.25 # set number or NaN\n",
    "\n",
    "if color_comb == 'white':\n",
    "    opposite_color = 'black'\n",
    "elif color_comb == 'black':\n",
    "    opposite_color = 'white'\n",
    "else:\n",
    "    raise ValueError('color_comb must be white or black')\n",
    "\n",
    "dataset = 'CAPS_K9'\n",
    "experiment = 'CAPS'\n",
    "figure_dims = (10, 1)\n",
    "project_dict = figure_utils.get_project_dict('K9')[0]\n",
    "figures_folder = figure_utils.get_path('Figures', project_dict, figure_letter=figure_letter)\n",
    "# define path to results\n",
    "res_path = \"C:\\github\\spatially_constrained_spectral_clustering\\results\\cluster_homogeinity\\homogeneity_K9.csv\"\n",
    "# setup path to save the table 'homogeneity_K9_per_N_clusters.csv'\n",
    "results_file = r\"C:\\github\\spatially_constrained_spectral_clustering\\results\\cluster_homogeinity\\homogeneity_K9_per_N_clusters.csv\"\n",
    "res_df = pd.read_csv(res_path)\n",
    "\n",
    "cluster_possible_list = res_df['Number of clusters'].unique()\n",
    "res_by_n_clusters = pd.DataFrame()\n",
    "for N_clusters in cluster_possible_list:\n",
    "    # Create a figure according to figure_dims\n",
    "    res_df_sel = res_df[res_df['Number of clusters'] == N_clusters]\n",
    "    new_row = {\n",
    "        'number_of_clusters': N_clusters,\n",
    "        'average_corr': np.nanmean(res_df_sel['average_corr'].values),\n",
    "        'nan_count': np.sum(np.isnan(res_df_sel['average_corr'].values)),\n",
    "    }\n",
    "    # add new_row to res_by_n_clusters\n",
    "    res_by_n_clusters = pd.concat([res_by_n_clusters, pd.DataFrame([new_row])], ignore_index=True)\n",
    " \n",
    "\n",
    "# save the results\n",
    "res_by_n_clusters.to_csv(results_file, index=False)\n",
    "print('Table saved to: ' + results_file)\n",
    "\n",
    "# create figure\n",
    "fig, ax = plt.subplots(figsize=figure_dims)\n",
    "# get the y values and x values\n",
    "y_values = res_by_n_clusters['average_corr'].values\n",
    "x_values = res_by_n_clusters['number_of_clusters'].values\n",
    "# plot the data\n",
    "ax.plot(x_values, y_values, 'o-', label='Data', color=opposite_color)\n",
    "\n",
    "# get min_y and max_y\n",
    "min_y = min(y_values)\n",
    "max_y = max(y_values)\n",
    "# define percentage to add to the y-axis\n",
    "perc = 0.1\n",
    "\n",
    "# add 10% of the y-axis\n",
    "if min_y < 0:\n",
    "    min_y = min_y + min_y*perc\n",
    "else:\n",
    "    min_y = min_y - min_y*perc\n",
    "if max_y < 0:\n",
    "    max_y = max_y - max_y*perc\n",
    "else:\n",
    "    max_y = max_y + max_y*perc\n",
    "\n",
    "\n",
    "# round min_y to the nearest 0.1\n",
    "min_y = round(min_y - 0.01, 2)\n",
    "\n",
    "# round max_y to the nearest 0.1\n",
    "max_y = round(max_y + 0.01, 2)\n",
    "\n",
    "# overwrite if limits are not NaN\n",
    "if not np.isnan(custom_min_y):\n",
    "    min_y = custom_min_y\n",
    "if not np.isnan(custom_max_y):\n",
    "    max_y = custom_max_y\n",
    "\n",
    "ax.set_ylim([min_y, max_y])\n",
    "\n",
    "# ─────────────── y-axis ticks & grid ───────────────\n",
    "# major ticks: just min and max\n",
    "ax.yaxis.set_major_locator(mticker.FixedLocator([min_y, max_y]))\n",
    "# minor ticks: every grid_spacing\n",
    "ax.yaxis.set_minor_locator(mticker.MultipleLocator(grid_spacing))\n",
    "# draw grid only at minor ticks\n",
    "ax.grid(which='minor', color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "# optionally turn off any major grid\n",
    "ax.grid(which='major', color='none')\n",
    "\n",
    "# ensure only major tick labels appear\n",
    "ax.tick_params(axis='y', which='minor', length=0)\n",
    "\n",
    "# Set the background color to none\n",
    "ax.set_facecolor('none')\n",
    "\n",
    "for spine in ['top','left','right','bottom']:\n",
    "    ax.spines[spine].set_visible(False)\n",
    "# hide x-axis ticks and labels\n",
    "ax.tick_params(axis='x', colors='none')\n",
    "# color the y-axis labels\n",
    "ax.tick_params(axis='y', colors=opposite_color)\n",
    "\n",
    "if second_axis:\n",
    "    # ------------------  second (right) axis – NEW scatter  ------------------\n",
    "    ax2 = ax.twinx()                              # create secondary y-axis\n",
    "    y_right = res_by_n_clusters['nan_count'].values\n",
    "    ax2.scatter(x_values, y_right, color='red', zorder=3,\n",
    "                label='NaN count', marker='o', s=40)\n",
    "\n",
    "    # Optional: give the right axis its own limits & style\n",
    "    perc = 0.1\n",
    "    min_y2, max_y2 = y_right.min(), y_right.max()\n",
    "    if min_y2 < 0:  min_y2 += min_y2*perc\n",
    "    else:           min_y2 -= min_y2*perc\n",
    "    if max_y2 < 0:  max_y2 -= max_y2*perc\n",
    "    else:           max_y2 += max_y2*perc\n",
    "    ax2.set_ylim(min_y2, max_y2)\n",
    "\n",
    "    ax2.tick_params(axis='y', colors='red')       # make tick labels red\n",
    "    ax2.spines['right'].set_visible(False)        # keep frameless look\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Determine path\n",
    "figure_path = os.path.join(figures_folder, 'Homogeneity_scatterplot' + '.png')\n",
    "if save_file:\n",
    "    # check that figures_folder exists, if not create it\n",
    "    if not os.path.exists(figures_folder):\n",
    "        os.makedirs(figures_folder)\n",
    "        print('Folder created: ' + figures_folder)\n",
    "\n",
    "\n",
    "    # Save the figure with transparency\n",
    "    fig.savefig(figure_path, transparent=True)\n",
    "    print('Figure saved to: ' + figure_path)\n",
    "else:\n",
    "    print(figure_path + ' not saved')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
