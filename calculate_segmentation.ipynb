{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "# path to cluster_roi_master module\n",
    "cluster_roi_master_folder = r\"C:\\github\\spatially_constrained_spectral_clustering\\cluster_roi_master\"\n",
    "# path to this repository\n",
    "segmentation_folder = r\"C:\\github\\spatially_constrained_spectral_clustering\"\n",
    "# add paths\n",
    "sys.path.append(cluster_roi_master_folder)\n",
    "sys.path.append(segmentation_folder)\n",
    "# import and reload figure_utils module\n",
    "import figure_utils\n",
    "\n",
    "# import specific functions from the cluster_roi_master module\n",
    "from make_local_connectivity_scorr import *\n",
    "from make_local_connectivity_tcorr import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7709 # of non-zero voxels in the mask\n",
      "Processing voxel # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing voxel # 1000\n",
      "Processing voxel # 2000\n",
      "Processing voxel # 3000\n",
      "Processing voxel # 4000\n",
      "Processing voxel # 5000\n",
      "Processing voxel # 6000\n",
      "Processing voxel # 7000\n",
      "Finished processing C:\\data\\CAPS_Knee\\normalized\\D-sub-02\\D-sub-02_ses-01_task-resting_state_run-01.nii.gz, total voxels processed: 7709\n",
      "7709 # of non-zero voxels in the mask\n",
      "Processing voxel # 0\n",
      "Processing voxel # 1000\n",
      "Processing voxel # 2000\n",
      "Processing voxel # 3000\n",
      "Processing voxel # 4000\n",
      "Processing voxel # 5000\n",
      "Processing voxel # 6000\n",
      "Processing voxel # 7000\n",
      "Finished processing C:\\data\\CAPS_Knee\\normalized\\D-sub-05\\D-sub-05_ses-01_task-resting_state_run-01.nii.gz, total voxels processed: 7709\n",
      "7709 # of non-zero voxels in the mask\n",
      "Processing voxel # 0\n",
      "Processing voxel # 1000\n",
      "Processing voxel # 2000\n",
      "Processing voxel # 3000\n",
      "Processing voxel # 4000\n",
      "Processing voxel # 5000\n",
      "Processing voxel # 6000\n",
      "Processing voxel # 7000\n",
      "Finished processing C:\\data\\CAPS_Knee\\normalized\\D-sub-06\\D-sub-06_ses-01_task-resting_state_run-01.nii.gz, total voxels processed: 7709\n",
      "7709 # of non-zero voxels in the mask\n",
      "Processing voxel # 0\n",
      "Processing voxel # 1000\n",
      "Processing voxel # 2000\n",
      "Processing voxel # 3000\n",
      "Processing voxel # 4000\n",
      "Processing voxel # 5000\n",
      "Processing voxel # 6000\n",
      "Processing voxel # 7000\n",
      "Finished processing C:\\data\\CAPS_Knee\\normalized\\D-sub-07\\D-sub-07_ses-01_task-resting_state_run-01.nii.gz, total voxels processed: 7709\n",
      "7709 # of non-zero voxels in the mask\n",
      "Processing voxel # 0\n",
      "Processing voxel # 1000\n",
      "Processing voxel # 2000\n",
      "Processing voxel # 3000\n",
      "Processing voxel # 4000\n",
      "Processing voxel # 5000\n",
      "Processing voxel # 6000\n",
      "Processing voxel # 7000\n",
      "Finished processing C:\\data\\CAPS_Knee\\normalized\\D-sub-09\\D-sub-09_ses-01_task-resting_state_run-01.nii.gz, total voxels processed: 7709\n",
      "7709 # of non-zero voxels in the mask\n",
      "Processing voxel # 0\n",
      "Processing voxel # 1000\n",
      "Processing voxel # 2000\n",
      "Processing voxel # 3000\n",
      "Processing voxel # 4000\n",
      "Processing voxel # 5000\n",
      "Processing voxel # 6000\n",
      "Processing voxel # 7000\n",
      "Finished processing C:\\data\\CAPS_Knee\\normalized\\D-sub-11\\D-sub-11_ses-01_task-resting_state_run-01.nii.gz, total voxels processed: 7709\n",
      "7709 # of non-zero voxels in the mask\n",
      "Processing voxel # 0\n",
      "Processing voxel # 1000\n",
      "Processing voxel # 2000\n",
      "Processing voxel # 3000\n",
      "Processing voxel # 4000\n",
      "Processing voxel # 5000\n",
      "Processing voxel # 6000\n",
      "Processing voxel # 7000\n",
      "Finished processing C:\\data\\CAPS_Knee\\normalized\\D-sub-12\\D-sub-12_ses-01_task-resting_state_run-01.nii.gz, total voxels processed: 7709\n",
      "7709 # of non-zero voxels in the mask\n",
      "Processing voxel # 0\n",
      "Processing voxel # 1000\n",
      "Processing voxel # 2000\n",
      "Processing voxel # 3000\n",
      "Processing voxel # 4000\n",
      "Processing voxel # 5000\n",
      "Processing voxel # 6000\n",
      "Processing voxel # 7000\n",
      "Finished processing C:\\data\\CAPS_Knee\\normalized\\D-sub-14\\D-sub-14_ses-01_task-resting_state_run-01.nii.gz, total voxels processed: 7709\n",
      "7709 # of non-zero voxels in the mask\n",
      "Processing voxel # 0\n",
      "Processing voxel # 1000\n",
      "Processing voxel # 2000\n",
      "Processing voxel # 3000\n",
      "Processing voxel # 4000\n",
      "Processing voxel # 5000\n",
      "Processing voxel # 6000\n",
      "Processing voxel # 7000\n",
      "Finished processing C:\\data\\CAPS_Knee\\normalized\\D-sub-16\\D-sub-16_ses-01_task-resting_state_run-01.nii.gz, total voxels processed: 7709\n",
      "7709 # of non-zero voxels in the mask\n",
      "Processing voxel # 0\n",
      "Processing voxel # 1000\n",
      "Processing voxel # 2000\n",
      "Processing voxel # 3000\n",
      "Processing voxel # 4000\n",
      "Processing voxel # 5000\n",
      "Processing voxel # 6000\n",
      "Processing voxel # 7000\n",
      "Finished processing C:\\data\\CAPS_Knee\\normalized\\D-sub-18\\D-sub-18_ses-01_task-resting_state_run-01.nii.gz, total voxels processed: 7709\n",
      "7709 # of non-zero voxels in the mask\n",
      "Processing voxel # 0\n",
      "Processing voxel # 1000\n",
      "Processing voxel # 2000\n",
      "Processing voxel # 3000\n",
      "Processing voxel # 4000\n",
      "Processing voxel # 5000\n",
      "Processing voxel # 6000\n",
      "Processing voxel # 7000\n",
      "Finished processing C:\\data\\CAPS_Knee\\normalized\\D-sub-21\\D-sub-21_ses-01_task-resting_state_run-01.nii.gz, total voxels processed: 7709\n",
      "7709 # of non-zero voxels in the mask\n",
      "Processing voxel # 0\n",
      "Processing voxel # 1000\n",
      "Processing voxel # 2000\n",
      "Processing voxel # 3000\n",
      "Processing voxel # 4000\n",
      "Processing voxel # 5000\n",
      "Processing voxel # 6000\n",
      "Processing voxel # 7000\n",
      "Finished processing C:\\data\\CAPS_Knee\\normalized\\D-sub-24\\D-sub-24_ses-01_task-resting_state_run-01.nii.gz, total voxels processed: 7709\n",
      "7709 # of non-zero voxels in the mask\n",
      "Processing voxel # 0\n",
      "Processing voxel # 1000\n",
      "Processing voxel # 2000\n",
      "Processing voxel # 3000\n",
      "Processing voxel # 4000\n",
      "Processing voxel # 5000\n",
      "Processing voxel # 6000\n",
      "Processing voxel # 7000\n",
      "Finished processing C:\\data\\CAPS_Knee\\normalized\\D-sub-27\\D-sub-27_ses-01_task-resting_state_run-01.nii.gz, total voxels processed: 7709\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Calculate hierarchical clustering at the individual level\n",
    "\n",
    "def pyClusterROI_corr(project_dict, sub_N, run_N, mask_name, corr_type, threshold, atlas_version='2mm'):\n",
    "    # Calculate temporal/spatial correlation, uses the pyClusterROI toolbox from Cameron Craddock (https://github.com/ccraddock/cluster_roi)\n",
    "    # project_dict: dictionary containing the project information\n",
    "    # sub_N: subject number\n",
    "    # run_N: run number\n",
    "    # mask_name: name of the mask, the mask will be loaded from the associated atlas folder indicated in project_dict\n",
    "    # corr_type: type of correlation ('t' for temporal or 's' for spatial)\n",
    "    # threshold: threshold for the correlation matrix\n",
    "    dataset = project_dict['Dataset']\n",
    "    session = project_dict['Session']\n",
    "    task = project_dict['Task']\n",
    "    specie = project_dict['Specie']\n",
    "    datafolder = project_dict['Datafolder']\n",
    "    \n",
    "    # input_folder has the normalized data\n",
    "    input_folder = datafolder + os.sep + dataset + os.sep + 'normalized' + os.sep + specie + '-sub-' + str(sub_N).zfill(2)\n",
    "    input_file = input_folder \n",
    "    input_file = input_file + os.sep + specie + '-sub-' + str(sub_N).zfill(2) + '_ses-' + session + '_task-' + task + '_run-' + str(run_N).zfill(2) + '.nii.gz'\n",
    "    # output_folder has the results of the clustering\n",
    "    output_folder = datafolder + os.sep + dataset + os.sep + 'hierarchical_clustering' + os.sep + specie + '-sub-' + str(sub_N).zfill(3)\n",
    "    output_file = output_folder \n",
    "    output_file = output_file +  os.sep + specie + '-sub-' + str(sub_N).zfill(3) + '_task-' + task + '_run-' + str(run_N).zfill(2) + '-' + corr_type + 'corr.npy'\n",
    "    # Create the output folder if it does not exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    maskfile = os.path.join(datafolder, dataset, 'masks', f'{mask_name}.nii.gz')\n",
    "    \n",
    "    # run make_local_connectivity based on the type of function \n",
    "    if corr_type == 't':\n",
    "        make_local_connectivity_tcorr(input_file, maskfile, output_file, threshold)\n",
    "    elif corr_type == 's':\n",
    "        make_local_connectivity_scorr(input_file, maskfile, output_file, threshold)\n",
    "    else:\n",
    "        print('Error: invalid correlation type, valid options are \"t\" for temporal or \"s\" for spatial')\n",
    "    \n",
    "\n",
    "# Change here to process a different dataset\n",
    "selected_dataset = 'Knee'\n",
    "project_dict,_ = figure_utils.get_project_dict(selected_dataset)\n",
    "datafolder = r\"C:\\data\"\n",
    "# assign to project dict\n",
    "project_dict['Datafolder'] = datafolder\n",
    "\n",
    "\n",
    "run_N = 1\n",
    "mask_name = 'b_GreyMatter2mm'\n",
    "threshold = 0.5\n",
    "corr_type = 't'\n",
    "\n",
    "for sub_N in project_dict['Participants']:\n",
    "    # run the function for each subject\n",
    "    pyClusterROI_corr(project_dict, sub_N, run_N, mask_name, corr_type, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_20.npy\n",
      "Started at 1760880132.8661323\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-sub-002\\D-sub-002_task-resting_state_run-01-tcorr.npy as a .npy file\n",
      "data_w [1.         0.50435544 1.         ... 1.         0.64379326 0.78911662]\n",
      "data_w shape (50066,)\n",
      "indices_i [   0   33    1 ... 7708 7660 7669]\n",
      "max indices_i 7708\n",
      "indices_j [   0    0    1 ... 7708 7708 7708]\n",
      "max indices_j 7708\n",
      "n_voxels 7709\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-sub-005\\D-sub-005_task-resting_state_run-01-tcorr.npy as a .npy file\n",
      "data_w [1.         1.         1.         ... 0.65737304 0.70815641 1.        ]\n",
      "data_w shape (33874,)\n",
      "indices_i [   0    1    2 ... 7705 7659 7708]\n",
      "max indices_i 7708\n",
      "indices_j [   0    1    2 ... 7708 7708 7708]\n",
      "max indices_j 7708\n",
      "n_voxels 7709\n",
      "Adding 1\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-sub-006\\D-sub-006_task-resting_state_run-01-tcorr.npy as a .npy file\n",
      "data_w [1.         0.55085121 1.         ... 0.58189138 0.70041678 1.        ]\n",
      "data_w shape (32767,)\n",
      "indices_i [   0   33    1 ... 7707 7705 7708]\n",
      "max indices_i 7708\n",
      "indices_j [   0    0    1 ... 7708 7708 7708]\n",
      "max indices_j 7708\n",
      "n_voxels 7709\n",
      "Adding 2\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-sub-007\\D-sub-007_task-resting_state_run-01-tcorr.npy as a .npy file\n",
      "data_w [1.         0.54203028 1.         ... 0.60238481 1.         0.62172913]\n",
      "data_w shape (47956,)\n",
      "indices_i [   0   33    1 ... 7659 7708 7706]\n",
      "max indices_i 7708\n",
      "indices_j [   0    0    1 ... 7708 7708 7708]\n",
      "max indices_j 7708\n",
      "n_voxels 7709\n",
      "Adding 3\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-sub-009\\D-sub-009_task-resting_state_run-01-tcorr.npy as a .npy file\n",
      "data_w [1.         1.         0.50669351 ... 0.65676513 0.67082621 1.        ]\n",
      "data_w shape (45037,)\n",
      "indices_i [   0    1   42 ... 7705 7659 7708]\n",
      "max indices_i 7708\n",
      "indices_j [   0    1    1 ... 7708 7708 7708]\n",
      "max indices_j 7708\n",
      "n_voxels 7709\n",
      "Adding 4\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-sub-011\\D-sub-011_task-resting_state_run-01-tcorr.npy as a .npy file\n",
      "data_w [1.         0.50483755 1.         ... 0.73264456 0.66274774 1.        ]\n",
      "data_w shape (37561,)\n",
      "indices_i [   0   33    1 ... 7705 7659 7708]\n",
      "max indices_i 7708\n",
      "indices_j [   0    0    1 ... 7708 7708 7708]\n",
      "max indices_j 7708\n",
      "n_voxels 7709\n",
      "Adding 5\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-sub-012\\D-sub-012_task-resting_state_run-01-tcorr.npy as a .npy file\n",
      "data_w [0.57534448 1.         1.         ... 0.68516228 0.56390915 0.57522147]\n",
      "data_w shape (62288,)\n",
      "indices_i [   1    0    1 ... 7668 7649 7706]\n",
      "max indices_i 7708\n",
      "indices_j [   0    0    1 ... 7708 7708 7708]\n",
      "max indices_j 7708\n",
      "n_voxels 7709\n",
      "Adding 6\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-sub-014\\D-sub-014_task-resting_state_run-01-tcorr.npy as a .npy file\n",
      "data_w [0.52596954 1.         1.         ... 0.55809742 0.52081791 1.        ]\n",
      "data_w shape (44960,)\n",
      "indices_i [   1    0    1 ... 7705 7659 7708]\n",
      "max indices_i 7708\n",
      "indices_j [   0    0    1 ... 7708 7708 7708]\n",
      "max indices_j 7708\n",
      "n_voxels 7709\n",
      "Adding 7\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-sub-018\\D-sub-018_task-resting_state_run-01-tcorr.npy as a .npy file\n",
      "data_w [1.         0.52203764 0.51667889 ... 0.60037863 0.62641422 1.        ]\n",
      "data_w shape (37162,)\n",
      "indices_i [   0   31   41 ... 7705 7659 7708]\n",
      "max indices_i 7708\n",
      "indices_j [   0    1    1 ... 7708 7708 7708]\n",
      "max indices_j 7708\n",
      "n_voxels 7709\n",
      "Adding 8\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-sub-027\\D-sub-027_task-resting_state_run-01-tcorr.npy as a .npy file\n",
      "data_w [1.         0.51498162 1.         ... 0.56628919 0.60493105 1.        ]\n",
      "data_w shape (33787,)\n",
      "indices_i [   0   33    1 ... 7705 7659 7708]\n",
      "max indices_i 7708\n",
      "indices_j [   0    0    1 ... 7708 7708 7708]\n",
      "max indices_j 7708\n",
      "n_voxels 7709\n",
      "Adding 9\n",
      "Finished reading data and calculating connectivity after 0.19446754455566406 seconds\n",
      "\n",
      "Finished calculating eigenvectors after 12.597898006439209 seconds\n",
      "\n",
      "Discretisation converged successfully.\n",
      "Finished discretisation for k = 20 after 12.627864122390747 seconds\n",
      "\n",
      "Finished clustering for k = 20 after 12.634901285171509 seconds\n",
      "\n",
      "Discretisation converged successfully.\n",
      "Finished discretisation for k = 40 after 14.631303548812866 seconds\n",
      "\n",
      "Finished clustering for k = 40 after 14.643401145935059 seconds\n",
      "\n",
      "Discretisation converged successfully.\n",
      "Finished discretisation for k = 60 after 15.997177362442017 seconds\n",
      "\n",
      "Finished clustering for k = 60 after 16.01239776611328 seconds\n",
      "\n",
      "Discretisation converged successfully.\n",
      "Finished discretisation for k = 80 after 17.961435794830322 seconds\n",
      "\n",
      "Finished clustering for k = 80 after 17.983027458190918 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\github\\CAPS\\cluster_roi_master\\python_ncut_lib.py:161: RuntimeWarning: SVD did not converge after maximum retries. Proceeding with the best result obtained.\n",
      "  warnings.warn(\"SVD did not converge after maximum retries. Proceeding with the best result obtained.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished discretisation for k = 100 after 26.418443202972412 seconds\n",
      "\n",
      "Finished clustering for k = 100 after 26.439993619918823 seconds\n",
      "\n",
      "Finished discretisation for k = 120 after 35.72533822059631 seconds\n",
      "\n",
      "Finished clustering for k = 120 after 35.752628326416016 seconds\n",
      "\n",
      "Finished discretisation for k = 140 after 48.984681367874146 seconds\n",
      "\n",
      "Finished clustering for k = 140 after 49.01491570472717 seconds\n",
      "\n",
      "Finished discretisation for k = 160 after 64.02410340309143 seconds\n",
      "\n",
      "Finished clustering for k = 160 after 64.05585169792175 seconds\n",
      "\n",
      "Finished discretisation for k = 180 after 82.20767402648926 seconds\n",
      "\n",
      "Finished clustering for k = 180 after 82.2444019317627 seconds\n",
      "\n",
      "Finished discretisation for k = 200 after 103.30474901199341 seconds\n",
      "\n",
      "Finished clustering for k = 200 after 103.36059951782227 seconds\n",
      "\n",
      "Finished discretisation for k = 220 after 127.37049722671509 seconds\n",
      "\n",
      "Finished clustering for k = 220 after 127.41813540458679 seconds\n",
      "\n",
      "Finished discretisation for k = 240 after 153.72050547599792 seconds\n",
      "\n",
      "Finished clustering for k = 240 after 153.77180528640747 seconds\n",
      "\n",
      "Finished discretisation for k = 260 after 184.96521067619324 seconds\n",
      "\n",
      "Finished clustering for k = 260 after 185.0216362476349 seconds\n",
      "\n",
      "Finished discretisation for k = 280 after 222.52553272247314 seconds\n",
      "\n",
      "Finished clustering for k = 280 after 222.58610463142395 seconds\n",
      "\n",
      "Finished discretisation for k = 300 after 264.8444106578827 seconds\n",
      "\n",
      "Finished clustering for k = 300 after 264.90465807914734 seconds\n",
      "\n",
      "Finished all computations after 264.90465807914734 seconds\n",
      "\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_20.npy as a .npy filetype\n",
      "Saved NIfTI image to C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_20.nii.gz\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_40.npy as a .npy filetype\n",
      "Saved NIfTI image to C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_40.nii.gz\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_60.npy as a .npy filetype\n",
      "Saved NIfTI image to C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_60.nii.gz\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_80.npy as a .npy filetype\n",
      "Saved NIfTI image to C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_80.nii.gz\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_100.npy as a .npy filetype\n",
      "Saved NIfTI image to C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_100.nii.gz\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_120.npy as a .npy filetype\n",
      "Saved NIfTI image to C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_120.nii.gz\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_140.npy as a .npy filetype\n",
      "Saved NIfTI image to C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_140.nii.gz\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_160.npy as a .npy filetype\n",
      "Saved NIfTI image to C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_160.nii.gz\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_180.npy as a .npy filetype\n",
      "Saved NIfTI image to C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_180.nii.gz\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_200.npy as a .npy filetype\n",
      "Saved NIfTI image to C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_200.nii.gz\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_220.npy as a .npy filetype\n",
      "Saved NIfTI image to C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_220.nii.gz\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_240.npy as a .npy filetype\n",
      "Saved NIfTI image to C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_240.nii.gz\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_260.npy as a .npy filetype\n",
      "Saved NIfTI image to C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_260.nii.gz\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_280.npy as a .npy filetype\n",
      "Saved NIfTI image to C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_280.nii.gz\n",
      "Reading C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_300.npy as a .npy filetype\n",
      "Saved NIfTI image to C:\\data\\CAPS_Knee\\hierarchical_clustering\\D-group-tcorr-b_GreyMatter2mm_300.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# Step 2. Hierarchical clustering group at the group level\n",
    "from importlib import reload\n",
    "import importlib\n",
    "import nibabel as nb\n",
    "import imp\n",
    "import os\n",
    "import connectivityR\n",
    "reload(connectivityR)\n",
    "import shutil\n",
    "import numpy as np  # Ensure numpy is imported\n",
    "\n",
    "reload(connectivityR)\n",
    "\n",
    "def load_module(module_name, file_path):\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    if spec is None:\n",
    "        raise ImportError(f\"Cannot find module {module_name} at {file_path}\")\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "# Dynamically load additional modules\n",
    "\n",
    "cluster_roi_master_path = r\"C:\\github\\spatially_constrained_spectral_clustering\\cluster_roi_master\"\n",
    "datafolder = r\"C:\\data\"\n",
    "\n",
    "group_mean_path = os.path.join(cluster_roi_master_path, 'group_mean_binfile_parcellation.py')\n",
    "make_image_path = os.path.join(cluster_roi_master_path, 'make_image_from_bin_renum.py')\n",
    "\n",
    "groupMean = load_module('group_mean_binfile_parcellation', group_mean_path)\n",
    "make_img = load_module('make_image_from_bin_renum', make_image_path)\n",
    "\n",
    "def group_mean(datafolder, subs_possible, runs_possible, corr_type, number_of_clusters, mask_name, dataset, specie, task='rest', rnd=False, rnd_range=[0,100], rnd_N_group=1):\n",
    "    # specie = 'D'\n",
    "    # The name of the maskfile that we will be using\n",
    "    corr_conn_files = []    \n",
    "    for sub_N in subs_possible:\n",
    "        # print(f\"sub_N: {sub_N}\")\n",
    "        for run_N in runs_possible:\n",
    "            if rnd:  # if rnd is True, we need to add the rnd number to the file name\n",
    "                input_folder = os.path.join(\n",
    "                    datafolder, dataset, 'hierarchical_clustering', 'rnd',\n",
    "                    f'{specie}-sub-{str(sub_N).zfill(3)}'\n",
    "                )\n",
    "                # sample within the rnd range\n",
    "                rnd_N = np.random.randint(rnd_range[0], rnd_range[1]+1)\n",
    "                # create the file name adding the rnd number\n",
    "                input_file = f'{specie}-sub-{str(sub_N).zfill(3)}_{str(rnd_N).zfill(4)}'\n",
    "            else:  # regular file name\n",
    "                input_folder = os.path.join(\n",
    "                    datafolder, dataset, 'hierarchical_clustering',\n",
    "                    f'{specie}-sub-{str(sub_N).zfill(3)}'\n",
    "                )\n",
    "                input_file = f'{specie}-sub-{str(sub_N).zfill(3)}'\n",
    "          \n",
    "            # non rnd version\n",
    "            input_file += f'_task-{task}_run-{str(run_N).zfill(2)}-{corr_type}corr.npy'\n",
    "\n",
    "            full_input_path = os.path.join(input_folder, input_file)\n",
    "            # print(f'added {full_input_path}')\n",
    "            corr_conn_files.append(full_input_path)\n",
    "\n",
    "    maskFile = os.path.join(datafolder, dataset, 'masks', f'{mask_name}.nii.gz')\n",
    "    # Load maskFile\n",
    "    mask = nb.load(maskFile)\n",
    "    mask_data = mask.get_fdata()\n",
    "    # Count the number of voxels in the mask\n",
    "    n_voxels = np.count_nonzero(mask_data)\n",
    "    if rnd:  # if rnd is True, we need to add the padded rnd_N_group number to the file name\n",
    "        file_out = os.path.join(\n",
    "            datafolder, dataset, 'hierarchical_clustering', 'rnd',\n",
    "            f'{specie}-group-{corr_type}corr-{mask_name}_{str(rnd_N_group).zfill(4)}'\n",
    "        )\n",
    "        # generate a list of all files that should be generated\n",
    "        list_of_files = [f'{file_out}_{str(K)}.npy' for K in number_of_clusters]\n",
    "        # check if all files exist\n",
    "        if all([os.path.exists(f) for f in list_of_files]):\n",
    "            print(f'All files {file_out} already exist, skipping')\n",
    "            return\n",
    "        else:\n",
    "            print(f'Processing {file_out}_{str(number_of_clusters[0])}.npy')\n",
    "    else:\n",
    "        file_out = os.path.join(\n",
    "            datafolder, dataset, 'hierarchical_clustering',\n",
    "            f'{specie}-group-{corr_type}corr-{mask_name}'\n",
    "        )\n",
    "        # print\n",
    "        print(f'Processing {file_out}_{str(number_of_clusters[0])}.npy')\n",
    "\n",
    "    # Call the group mean function\n",
    "    groupMean.group_mean_binfile_parcellate(corr_conn_files, file_out, number_of_clusters, n_voxels)\n",
    "\n",
    "    for K in number_of_clusters:\n",
    "        if rnd:\n",
    "            npyFile = os.path.join(\n",
    "                datafolder, dataset, 'hierarchical_clustering', 'rnd',\n",
    "                f'{specie}-group-{corr_type}corr-{mask_name}_{str(rnd_N_group).zfill(4)}_{K}'\n",
    "            )\n",
    "        else:\n",
    "            npyFile = os.path.join(\n",
    "                datafolder, dataset, 'hierarchical_clustering',\n",
    "                f'{specie}-group-{corr_type}corr-{mask_name}_{K}'\n",
    "            )\n",
    "        # Generate NIfTI image from binary renumbered data\n",
    "        make_img.make_image_from_bin_renum(f'{npyFile}.nii.gz', f'{npyFile}.npy', maskFile)\n",
    "\n",
    "        # Load NIfTI file\n",
    "        img = nb.load(f'{npyFile}.nii.gz')\n",
    "        img_shape = img.shape\n",
    "        img_data = img.get_fdata().flatten()\n",
    "\n",
    "        # Check if folder matching npyFile exists, if it does, remove it\n",
    "        if os.path.exists(npyFile):\n",
    "            if os.path.isdir(npyFile):\n",
    "                shutil.rmtree(npyFile)\n",
    "            else:\n",
    "                os.remove(npyFile)\n",
    "        os.makedirs(npyFile)\n",
    "\n",
    "        # Get unique clusters\n",
    "        cluster_N = sorted(set(img_data[img_data > 0]))\n",
    "        for cluster in cluster_N:\n",
    "            cluster = int(cluster)\n",
    "            # Folder for the cluster\n",
    "            cluster_file = os.path.join(npyFile, f'cluster_{str(cluster).zfill(3)}.nii.gz')\n",
    "            cluster_img = np.zeros_like(img_data)\n",
    "            cluster_img[img_data == cluster] = 1\n",
    "\n",
    "            cluster_img = cluster_img.reshape(img_shape)\n",
    "            cluster_img_nifti = nb.Nifti1Image(cluster_img, img.affine)\n",
    "            nb.save(cluster_img_nifti, cluster_file)\n",
    "\n",
    "\n",
    "mask_name = 'b_GreyMatter2mm'\n",
    "corr_type = 't'\n",
    "specie = 'D'\n",
    "dataset = 'CAPS_Knee'\n",
    "task = 'resting_state'\n",
    "if dataset == 'CAPS_K9':\n",
    "    subs_possible = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,18,19,22,23,27,28,30,31,34,36,37]\n",
    "elif dataset == 'CAPS_epi':\n",
    "    subs_possible = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,18,19,22,23,27,28,30,31,34,36,37]\n",
    "elif dataset == 'CAPS_Knee':\n",
    "    subs_possible = [2,5,6,7,9,11,12,14,18,27]\n",
    "else:\n",
    "    # issue exception\n",
    "    raise ValueError(f\"Unknown dataset: {dataset}\")\n",
    "\n",
    "runs_possible = [1]\n",
    "number_of_clusters = [20,40,60,80,100,120,140,160,180,200,220,240,260,280,300]\n",
    "\n",
    "group_mean(datafolder,subs_possible,runs_possible,corr_type,number_of_clusters,mask_name,dataset, specie ,task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
